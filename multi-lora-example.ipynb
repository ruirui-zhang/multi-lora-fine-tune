{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rraydata/multi-lora-example?scriptVersionId=147764879\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fine-Tuning with Llama 2, ASPEN\n\nToday we'll explore fine-tuning the Llama 2 model available on Kaggle Models using Multi-lora.\n- Multi-lora: [ASPEN: Efficient LLM Model Fine-tune and Inference via Multi-Lora Optimization](https://github.com/TUDB-Labs/multi-lora-fine-tune#experiment-results) - ASPEN is an open-source framework for fine-tuning Large Language Models (LLMs) using the efficient multiple LoRA/QLoRA methods. Key features of ASPEN include: 1. Efficient LoRA/QLoRA: ASPEN optimizes the fine-tuning process, significantly reducing GPU memory usage by leveraging a shared frozen-based model.2. Multiple LoRA Adapters: Support for concurrent fine-tuning of multiple LoRA/qLoRA adapters.\n- QloRA: [Quantized Low Rank Adapters](https://arxiv.org/abs/2305.14314): This is a method for adjusting the LLMs that uses a small number of quantified and updatable parameters to limit the complexity of the training. This technique also allows these small sets of parameters to be efficiently added to the model itself, meaning that you can make fine-tuning adjustments on many datasets, potentially, and swap these \"adapters\" in your model when necessary.","metadata":{}},{"cell_type":"markdown","source":"## 1. Clone multi-lora repository","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:19:49.494429Z","iopub.execute_input":"2023-10-24T00:19:49.494695Z","iopub.status.idle":"2023-10-24T00:19:49.504597Z","shell.execute_reply.started":"2023-10-24T00:19:49.494669Z","shell.execute_reply":"2023-10-24T00:19:49.503833Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/TUDB-Labs/multi-lora-fine-tune.git","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:35:27.661693Z","iopub.execute_input":"2023-10-24T00:35:27.662521Z","iopub.status.idle":"2023-10-24T00:35:29.526325Z","shell.execute_reply.started":"2023-10-24T00:35:27.662483Z","shell.execute_reply":"2023-10-24T00:35:29.525356Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'multi-lora-fine-tune'...\nremote: Enumerating objects: 615, done.\u001b[K\nremote: Counting objects: 100% (302/302), done.\u001b[K\nremote: Compressing objects: 100% (113/113), done.\u001b[K\nremote: Total 615 (delta 197), reused 250 (delta 186), pack-reused 313\u001b[K\nReceiving objects: 100% (615/615), 2.21 MiB | 12.12 MiB/s, done.\nResolving deltas: 100% (327/327), done.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/working/multi-lora-fine-tune/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:35:36.343365Z","iopub.execute_input":"2023-10-24T00:35:36.343723Z","iopub.status.idle":"2023-10-24T00:38:44.670366Z","shell.execute_reply.started":"2023-10-24T00:35:36.343691Z","shell.execute_reply":"2023-10-24T00:38:44.669257Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torch==2.0.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting einops==0.6.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 2))\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.21.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3))\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers==4.31.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4))\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.40.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 5))\n  Downloading bitsandbytes-0.40.0-py3-none-any.whl (91.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 6)) (0.1.99)\nCollecting scipy==1.10.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 7))\n  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting xformers==0.0.20 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\n  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (3.2.4)\nRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 10)) (0.42.1)\nCollecting rouge (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 11))\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nCollecting rouge_chinese (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 12))\n  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m618.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (4.66.1)\nCollecting pyre-extensions==0.0.29 (from xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\n  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (0.40.0)\nRequirement already satisfied: typing-inspect in /opt/conda/lib/python3.10/site-packages (from pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (0.9.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\n  Downloading lit-17.0.3.tar.gz (154 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (1.0.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93257 sha256=e217a175f14a9284503288c83cd076f8faeec1ffc318cfb7bc88355683f23d1b\n  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\nSuccessfully built lit\nInstalling collected packages: lit, cmake, bitsandbytes, scipy, rouge_chinese, rouge, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, einops, pyre-extensions, nvidia-cusolver-cu11, nvidia-cudnn-cu11, transformers, triton, torch, xformers, accelerate\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.2\n    Uninstalling scipy-1.11.2:\n      Successfully uninstalled scipy-1.11.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.22.0\n    Uninstalling accelerate-0.22.0:\n      Successfully uninstalled accelerate-0.22.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 2.0.2 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.0 cmake-3.27.7 einops-0.6.1 lit-17.0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 rouge-1.0.1 rouge_chinese-1.0.3 scipy-1.10.1 torch-2.0.1 transformers-4.31.0 triton-2.0.0 xformers-0.0.20\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Config finetune datasets and parameters. You can add multiple lora parameters and datasets.","metadata":{}},{"cell_type":"markdown","source":"ASPEN can be used on:\n\n1) Domain-Specific Fine-Tuning:  This involves adapting a single model with various parameters particularly for one domain.\n\n2) Cross-Domain Fine-Tuning: This approach utilizes the foundational model to optimize multiple models, each designed for diverse domains, by incorporating datasets from various or identical domains.\n\nThe demo data and prompt are for demonstration purpose.","metadata":{}},{"cell_type":"code","source":"!cat /kaggle/working/multi-lora-fine-tune/data/data_demo.json","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:41:05.633951Z","iopub.execute_input":"2023-10-24T00:41:05.63432Z","iopub.status.idle":"2023-10-24T00:41:06.594461Z","shell.execute_reply.started":"2023-10-24T00:41:05.634287Z","shell.execute_reply":"2023-10-24T00:41:06.593475Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[\n    {\n        \"instruction\": \"Instruction demo.\",\n        \"input\": \"Input demo.\",\n        \"output\": \"Output demo.\"\n    },\n    {\n        \"instruction\": \"Instruction demo.\",\n        \"output\": \"Output demo.\"\n    }\n]","output_type":"stream"}]},{"cell_type":"code","source":"!cat /kaggle/working/multi-lora-fine-tune/template/template_demo.json","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:41:08.645034Z","iopub.execute_input":"2023-10-24T00:41:08.645388Z","iopub.status.idle":"2023-10-24T00:41:09.609506Z","shell.execute_reply.started":"2023-10-24T00:41:08.645357Z","shell.execute_reply":"2023-10-24T00:41:09.608431Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{\n    \"description\": \"\",\n    \"parameter\": [\n        \"input\",\n        \"output\",\n        \"instruction\"\n    ],\n    \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n{output}\\n\",\n    \"prompt_no_input\": \"### Instruction:\\n{instruction}\\n\\n### Output:\\n{output}\\n\"\n}","output_type":"stream"}]},{"cell_type":"code","source":"config_string = \"\"\"\n{\n    \"cutoff_len\": 256,\n    \"group_by_length\": false,\n    \"expand_right\": true,\n    \"pad_token_id\": -1,\n    \"save_step\": 2000,\n    \"early_stop_test_step\": 2000,\n    \"train_lora_candidate_num\": 4,\n    \"train_lora_simultaneously_num\": 2,\n    \"train_strategy\": \"optim\",\n    \"lora\": [\n        {\n            \"name\": \"lora_0\",\n            \"output\": \"lora_0\",\n            \"optim\": \"adamw\",\n            \"lr\": 3e-4,\n            \"batch_size\": 16,\n            \"micro_batch_size\": 4,\n            \"test_batch_size\": 64,\n            \"num_epochs\": 3,\n            \"r\": 8,\n            \"alpha\": 16,\n            \"dropout\": 0.05,\n            \"target_modules\": {\n                \"q_proj\": true,\n                \"k_proj\": false,\n                \"v_proj\": true,\n                \"o_proj\": false,\n                \"w1_proj\": false,\n                \"w2_proj\": false,\n                \"w3_proj\": false\n            },\n            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n        },\n        {\n            \"name\": \"lora_1\",\n            \"output\": \"lora_1\",\n            \"optim\": \"adamw\",\n            \"lr\": 3e-4,\n            \"batch_size\": 16,\n            \"micro_batch_size\": 4,\n            \"test_batch_size\": 64,\n            \"num_epochs\": 3,\n            \"r\": 8,\n            \"alpha\": 16,\n            \"dropout\": 0.05,\n            \"target_modules\": {\n                \"q_proj\": true,\n                \"k_proj\": false,\n                \"v_proj\": true,\n                \"o_proj\": false,\n                \"w1_proj\": false,\n                \"w2_proj\": false,\n                \"w3_proj\": false\n            },\n            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n        }\n    ]\n}\n\"\"\"\n\nwith open(\"./config.json\", \"w\") as f:\n    f.write(config_string)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:46:11.112535Z","iopub.execute_input":"2023-10-24T00:46:11.113452Z","iopub.status.idle":"2023-10-24T00:46:11.120577Z","shell.execute_reply.started":"2023-10-24T00:46:11.1134Z","shell.execute_reply":"2023-10-24T00:46:11.119599Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":" ## 4. Add the path of the base model and config file path to start finetune","metadata":{}},{"cell_type":"markdown","source":"remember to check whether the section is on GPU. ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/multi-lora-fine-tune/mlora.py \\\n  --base_model /kaggle/input/llama-2/pytorch/7b-hf/1 \\\n  --config /kaggle/working/config.json \\\n  --load_8bit","metadata":{"execution":{"iopub.status.busy":"2023-10-24T00:46:13.853866Z","iopub.execute_input":"2023-10-24T00:46:13.854206Z","iopub.status.idle":"2023-10-24T00:46:58.45616Z","shell.execute_reply.started":"2023-10-24T00:46:13.854179Z","shell.execute_reply":"2023-10-24T00:46:58.455076Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n[2023-10-24 00:46:22] ASPEN: NVIDIA CUDA initialized successfully.\n[2023-10-24 00:46:22] ASPEN: Total 2 GPU(s) detected.\n[2023-10-24 00:46:22] ASPEN: Loading model with quantization, bits = 8\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:26<00:00, 13.39s/it]\nencode text data lora_0: 0/2\nencode text data lora_0: 0/2\nencode text data lora_1: 0/2\nencode text data lora_1: 0/2\nlora_1 train data:\n    epoch: 1/3             step in epoch: 0/2\nlora_0 train data:\n    epoch: 1/3             step in epoch: 0/2\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n    adapter: lora_1 loss: 1.8237963914871216\n    adapter: lora_0 loss: 1.8237965106964111\nlora_0 train data:\n    epoch: 2/3             step in epoch: 0/2\nlora_1 train data:\n    epoch: 2/3             step in epoch: 0/2\n    adapter: lora_0 loss: 1.5725919008255005\n    adapter: lora_1 loss: 1.5725919008255005\nlora_0 train data:\n    epoch: 3/3             step in epoch: 0/2\nlora_1 train data:\n    epoch: 3/3             step in epoch: 0/2\n    adapter: lora_0 loss: 1.5725919008255005\n    adapter: lora_1 loss: 1.5725919008255005\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Then two files(lora_0, lora_1) appear in the current directory, that's the finetuned model adapter. We can download them.","metadata":{}}]}