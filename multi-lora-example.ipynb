{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rraydata/use-less-gpu-resource-to-fine-tune-llama-and-llama?scriptVersionId=147887099\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Use less GPU resource to Fine Tune LLAMA and LLAMA2 \n\nToday we'll explore fine-tuning the Llama 2 model available on Kaggle Models using Multi-lora.\n\n-  LoRa: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685): freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.\n- QloRA: [Quantized Low Rank Adapters](https://arxiv.org/abs/2305.14314):QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA).While the base model is quantized with NF4, the trained LoRAâ€™s parameters remain at a higher precision which is usually FP16. \n\n\nDisadvantages of LoRA-based Approaches:\n\n- Memory Consumption: Some Lora techniques might be efficient, but the introduction of low-rank approximations can sometimes heighten memory usage, particularly if one has to store both original and approximated parameters.\n- Potential for Reduced Model Accuracy: Lora-based fine-tuning is designed to either maintain or boost model accuracy. However, there can be instances where the approximations cause a dip in performance, especially if the low-rank approximations aren't chosen carefully.\n- Dependence on Hyperparameters: Much like other ML techniques, Lora-based strategies involve hyperparameters that need precise fine-tuning. Mistakes in this area can lead to subpar performance.\n\n\n[ASPEN: Efficient LLM Model Fine-tune and Inference via Multi-Lora Optimization](https://github.com/TUDB-Labs/multi-lora-fine-tune#experiment-results) is an open-source framework for fine-tuning Large Language Models (LLMs) using the efficient multiple LoRA/QLoRA methods. How Multi-lora improve upon LoRA-based approaches:\n\n- GPU Memory Conservation: Use one foundational model for multiple fine tuning process, significantly saving resources.\n- Automatic Parameter Learning: Introducing automation in the learning process for hyperparameters during model fine-tuning can speed up the process and guarantee optimal model results.\n- Early Stopping Mechanism: Implementing this approach ensures no overfitting occurs, and resources are utilized effectively. It stops training once the model's improvement becomes negligible.","metadata":{}},{"cell_type":"markdown","source":"## 1. Clone multi-lora repository","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/TUDB-Labs/multi-lora-fine-tune.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/working/multi-lora-fine-tune/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Config finetune datasets and parameters. You can add multiple lora parameters and datasets.","metadata":{}},{"cell_type":"markdown","source":"ASPEN can be used on:\n\n1) Domain-Specific Fine-Tuning:  This involves adapting a single model with various parameters particularly for one domain.\n\n2) Cross-Domain Fine-Tuning: This approach utilizes the foundational model to optimize multiple models, each designed for diverse domains, by incorporating datasets from various or identical domains.\n\nThe demo data and prompt are for demonstration purpose.","metadata":{}},{"cell_type":"code","source":"!cat /kaggle/working/multi-lora-fine-tune/data/data_demo.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat /kaggle/working/multi-lora-fine-tune/template/template_demo.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_string = \"\"\"\n{\n    \"cutoff_len\": 256,\n    \"group_by_length\": false,\n    \"expand_right\": true,\n    \"pad_token_id\": -1,\n    \"save_step\": 2000,\n    \"early_stop_test_step\": 2000,\n    \"train_lora_candidate_num\": 4,\n    \"train_lora_simultaneously_num\": 2,\n    \"train_strategy\": \"optim\",\n    \"lora\": [\n        {\n            \"name\": \"lora_0\",\n            \"output\": \"lora_0\",\n            \"optim\": \"adamw\",\n            \"lr\": 3e-4,\n            \"batch_size\": 16,\n            \"micro_batch_size\": 4,\n            \"test_batch_size\": 64,\n            \"num_epochs\": 3,\n            \"r\": 8,\n            \"alpha\": 16,\n            \"dropout\": 0.05,\n            \"target_modules\": {\n                \"q_proj\": true,\n                \"k_proj\": false,\n                \"v_proj\": true,\n                \"o_proj\": false,\n                \"w1_proj\": false,\n                \"w2_proj\": false,\n                \"w3_proj\": false\n            },\n            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n        },\n        {\n            \"name\": \"lora_1\",\n            \"output\": \"lora_1\",\n            \"optim\": \"adamw\",\n            \"lr\": 3e-4,\n            \"batch_size\": 16,\n            \"micro_batch_size\": 4,\n            \"test_batch_size\": 64,\n            \"num_epochs\": 3,\n            \"r\": 8,\n            \"alpha\": 16,\n            \"dropout\": 0.05,\n            \"target_modules\": {\n                \"q_proj\": true,\n                \"k_proj\": false,\n                \"v_proj\": true,\n                \"o_proj\": false,\n                \"w1_proj\": false,\n                \"w2_proj\": false,\n                \"w3_proj\": false\n            },\n            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n        }\n    ]\n}\n\"\"\"\n\nwith open(\"./config.json\", \"w\") as f:\n    f.write(config_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## 4. Add the path of the base model and config file path to start finetune","metadata":{}},{"cell_type":"markdown","source":"remember to check whether the section is on GPU. ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/multi-lora-fine-tune/mlora.py \\\n  --base_model /kaggle/input/llama-2/pytorch/7b-hf/1 \\\n  --config /kaggle/working/config.json \\\n  --load_8bit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Then two files(lora_0, lora_1) appear in the current directory, that's the finetuned model adapter. We can download them.","metadata":{}}]}