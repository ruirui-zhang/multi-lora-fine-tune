{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rraydata/multi-lora-example?scriptVersionId=147886659\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"8bbf1a2d","metadata":{"papermill":{"duration":0.004267,"end_time":"2023-10-24T23:14:33.675284","exception":false,"start_time":"2023-10-24T23:14:33.671017","status":"completed"},"tags":[]},"source":["# Use less GPU resource to Fine Tune LLAMA and LLAMA2 \n","\n","Today we'll explore fine-tuning the Llama 2 model available on Kaggle Models using Multi-lora.\n","\n","-  LoRa: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685): freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.\n","- QloRA: [Quantized Low Rank Adapters](https://arxiv.org/abs/2305.14314):QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA).While the base model is quantized with NF4, the trained LoRA’s parameters remain at a higher precision which is usually FP16. \n","\n","\n","Disadvantages of LoRA-based Approaches:\n","\n","- Memory Consumption: Some Lora techniques might be efficient, but the introduction of low-rank approximations can sometimes heighten memory usage, particularly if one has to store both original and approximated parameters.\n","- Potential for Reduced Model Accuracy: Lora-based fine-tuning is designed to either maintain or boost model accuracy. However, there can be instances where the approximations cause a dip in performance, especially if the low-rank approximations aren't chosen carefully.\n","- Dependence on Hyperparameters: Much like other ML techniques, Lora-based strategies involve hyperparameters that need precise fine-tuning. Mistakes in this area can lead to subpar performance.\n","\n","\n","[ASPEN: Efficient LLM Model Fine-tune and Inference via Multi-Lora Optimization](https://github.com/TUDB-Labs/multi-lora-fine-tune#experiment-results) is an open-source framework for fine-tuning Large Language Models (LLMs) using the efficient multiple LoRA/QLoRA methods. How Multi-lora improve upon LoRA-based approaches:\n","\n","- GPU Memory Conservation: Use one foundational model for multiple fine tuning process, significantly saving resources.\n","- Automatic Parameter Learning: Introducing automation in the learning process for hyperparameters during model fine-tuning can speed up the process and guarantee optimal model results.\n","- Early Stopping Mechanism: Implementing this approach ensures no overfitting occurs, and resources are utilized effectively. It stops training once the model's improvement becomes negligible."]},{"cell_type":"markdown","id":"47808261","metadata":{"papermill":{"duration":0.003551,"end_time":"2023-10-24T23:14:33.682718","exception":false,"start_time":"2023-10-24T23:14:33.679167","status":"completed"},"tags":[]},"source":["## 1. Clone multi-lora repository"]},{"cell_type":"code","execution_count":1,"id":"4fb3a8c6","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:14:33.69169Z","iopub.status.busy":"2023-10-24T23:14:33.691346Z","iopub.status.idle":"2023-10-24T23:14:33.701967Z","shell.execute_reply":"2023-10-24T23:14:33.701023Z"},"papermill":{"duration":0.017422,"end_time":"2023-10-24T23:14:33.704003","exception":false,"start_time":"2023-10-24T23:14:33.686581","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","os.chdir('/kaggle/working/')"]},{"cell_type":"code","execution_count":2,"id":"2e06270f","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:14:33.713248Z","iopub.status.busy":"2023-10-24T23:14:33.712956Z","iopub.status.idle":"2023-10-24T23:14:35.511801Z","shell.execute_reply":"2023-10-24T23:14:35.510719Z"},"papermill":{"duration":1.806086,"end_time":"2023-10-24T23:14:35.514273","exception":false,"start_time":"2023-10-24T23:14:33.708187","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'multi-lora-fine-tune'...\r\n","remote: Enumerating objects: 621, done.\u001b[K\r\n","remote: Counting objects: 100% (308/308), done.\u001b[K\r\n","remote: Compressing objects: 100% (116/116), done.\u001b[K\r\n","remote: Total 621 (delta 202), reused 253 (delta 189), pack-reused 313\u001b[K\r\n","Receiving objects: 100% (621/621), 2.21 MiB | 12.81 MiB/s, done.\r\n","Resolving deltas: 100% (332/332), done.\r\n"]}],"source":["!git clone https://github.com/TUDB-Labs/multi-lora-fine-tune.git"]},{"cell_type":"markdown","id":"6ea04042","metadata":{"papermill":{"duration":0.004627,"end_time":"2023-10-24T23:14:35.52419","exception":false,"start_time":"2023-10-24T23:14:35.519563","status":"completed"},"tags":[]},"source":["## 2. Install dependencies"]},{"cell_type":"code","execution_count":3,"id":"3caece31","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:14:35.535149Z","iopub.status.busy":"2023-10-24T23:14:35.534849Z","iopub.status.idle":"2023-10-24T23:17:44.414002Z","shell.execute_reply":"2023-10-24T23:17:44.413006Z"},"papermill":{"duration":188.887504,"end_time":"2023-10-24T23:17:44.41637","exception":false,"start_time":"2023-10-24T23:14:35.528866","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch==2.0.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting einops==0.6.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 2))\r\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting accelerate==0.21.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3))\r\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting transformers==4.31.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4))\r\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting bitsandbytes==0.40.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 5))\r\n","  Downloading bitsandbytes-0.40.0-py3-none-any.whl (91.9 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 6)) (0.1.99)\r\n","Collecting scipy==1.10.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 7))\r\n","  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting xformers==0.0.20 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\r\n","  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (3.2.4)\r\n","Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 10)) (0.42.1)\r\n","Collecting rouge (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 11))\r\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\r\n","Collecting rouge_chinese (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 12))\r\n","  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.12.2)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (4.6.3)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.12)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1.2)\r\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (1.23.5)\r\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (21.3)\r\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (5.9.3)\r\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (6.0)\r\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.16.4)\r\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.6.3)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2.31.0)\r\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.13.3)\r\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.3.3)\r\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (4.66.1)\r\n","Collecting pyre-extensions==0.0.29 (from xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\r\n","  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\r\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (68.0.0)\r\n","Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (0.40.0)\r\n","Requirement already satisfied: typing-inspect in /opt/conda/lib/python3.10/site-packages (from pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (0.9.0)\r\n","Collecting cmake (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading lit-17.0.3.tar.gz (154 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (1.16.0)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.9.0)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (3.0.9)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.1.0)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.4)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (1.26.15)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.7.22)\r\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.3.0)\r\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (1.0.0)\r\n","Building wheels for collected packages: lit\r\n","  Building wheel for lit (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93257 sha256=4fa0a0b15285166b3560a22a3ac4aff0b92e4f4659c0ad79f1673856c76d31e3\r\n","  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\r\n","Successfully built lit\r\n","Installing collected packages: lit, cmake, bitsandbytes, scipy, rouge_chinese, rouge, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, einops, pyre-extensions, nvidia-cusolver-cu11, nvidia-cudnn-cu11, transformers, triton, torch, xformers, accelerate\r\n","  Attempting uninstall: scipy\r\n","    Found existing installation: scipy 1.11.2\r\n","    Uninstalling scipy-1.11.2:\r\n","      Successfully uninstalled scipy-1.11.2\r\n","  Attempting uninstall: transformers\r\n","    Found existing installation: transformers 4.33.0\r\n","    Uninstalling transformers-4.33.0:\r\n","      Successfully uninstalled transformers-4.33.0\r\n","  Attempting uninstall: torch\r\n","    Found existing installation: torch 2.0.0\r\n","    Uninstalling torch-2.0.0:\r\n","      Successfully uninstalled torch-2.0.0\r\n","  Attempting uninstall: accelerate\r\n","    Found existing installation: accelerate 0.22.0\r\n","    Uninstalling accelerate-0.22.0:\r\n","      Successfully uninstalled accelerate-0.22.0\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n","fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 2.0.2 which is incompatible.\r\n","momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\r\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\r\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.0 cmake-3.27.7 einops-0.6.1 lit-17.0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 rouge-1.0.1 rouge_chinese-1.0.3 scipy-1.10.1 torch-2.0.1 transformers-4.31.0 triton-2.0.0 xformers-0.0.20\r\n"]}],"source":["!pip install -r /kaggle/working/multi-lora-fine-tune/requirements.txt"]},{"cell_type":"markdown","id":"029110e1","metadata":{"papermill":{"duration":0.070954,"end_time":"2023-10-24T23:17:44.558457","exception":false,"start_time":"2023-10-24T23:17:44.487503","status":"completed"},"tags":[]},"source":["## 3. Config finetune datasets and parameters. You can add multiple lora parameters and datasets."]},{"cell_type":"markdown","id":"cf3d12ad","metadata":{"papermill":{"duration":0.074091,"end_time":"2023-10-24T23:17:44.704009","exception":false,"start_time":"2023-10-24T23:17:44.629918","status":"completed"},"tags":[]},"source":["ASPEN can be used on:\n","\n","1) Domain-Specific Fine-Tuning:  This involves adapting a single model with various parameters particularly for one domain.\n","\n","2) Cross-Domain Fine-Tuning: This approach utilizes the foundational model to optimize multiple models, each designed for diverse domains, by incorporating datasets from various or identical domains.\n","\n","The demo data and prompt are for demonstration purpose."]},{"cell_type":"code","execution_count":4,"id":"969ed348","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:17:44.854109Z","iopub.status.busy":"2023-10-24T23:17:44.853399Z","iopub.status.idle":"2023-10-24T23:17:45.829781Z","shell.execute_reply":"2023-10-24T23:17:45.828414Z"},"papermill":{"duration":1.053769,"end_time":"2023-10-24T23:17:45.833123","exception":false,"start_time":"2023-10-24T23:17:44.779354","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[\r\n","    {\r\n","        \"instruction\": \"Instruction demo.\",\r\n","        \"input\": \"Input demo.\",\r\n","        \"output\": \"Output demo.\"\r\n","    },\r\n","    {\r\n","        \"instruction\": \"Instruction demo.\",\r\n","        \"output\": \"Output demo.\"\r\n","    }\r\n","]"]}],"source":["!cat /kaggle/working/multi-lora-fine-tune/data/data_demo.json"]},{"cell_type":"code","execution_count":5,"id":"8e1e44aa","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:17:45.983723Z","iopub.status.busy":"2023-10-24T23:17:45.983363Z","iopub.status.idle":"2023-10-24T23:17:47.006195Z","shell.execute_reply":"2023-10-24T23:17:47.004993Z"},"papermill":{"duration":1.098587,"end_time":"2023-10-24T23:17:47.008622","exception":false,"start_time":"2023-10-24T23:17:45.910035","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{\r\n","    \"description\": \"\",\r\n","    \"parameter\": [\r\n","        \"input\",\r\n","        \"output\",\r\n","        \"instruction\"\r\n","    ],\r\n","    \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n{output}\\n\",\r\n","    \"prompt_no_input\": \"### Instruction:\\n{instruction}\\n\\n### Output:\\n{output}\\n\"\r\n","}"]}],"source":["!cat /kaggle/working/multi-lora-fine-tune/template/template_demo.json"]},{"cell_type":"code","execution_count":6,"id":"8acecd1a","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:17:47.155771Z","iopub.status.busy":"2023-10-24T23:17:47.154792Z","iopub.status.idle":"2023-10-24T23:17:47.163384Z","shell.execute_reply":"2023-10-24T23:17:47.16224Z"},"papermill":{"duration":0.08518,"end_time":"2023-10-24T23:17:47.165633","exception":false,"start_time":"2023-10-24T23:17:47.080453","status":"completed"},"tags":[]},"outputs":[],"source":["config_string = \"\"\"\n","{\n","    \"cutoff_len\": 256,\n","    \"group_by_length\": false,\n","    \"expand_right\": true,\n","    \"pad_token_id\": -1,\n","    \"save_step\": 2000,\n","    \"early_stop_test_step\": 2000,\n","    \"train_lora_candidate_num\": 4,\n","    \"train_lora_simultaneously_num\": 2,\n","    \"train_strategy\": \"optim\",\n","    \"lora\": [\n","        {\n","            \"name\": \"lora_0\",\n","            \"output\": \"lora_0\",\n","            \"optim\": \"adamw\",\n","            \"lr\": 3e-4,\n","            \"batch_size\": 16,\n","            \"micro_batch_size\": 4,\n","            \"test_batch_size\": 64,\n","            \"num_epochs\": 3,\n","            \"r\": 8,\n","            \"alpha\": 16,\n","            \"dropout\": 0.05,\n","            \"target_modules\": {\n","                \"q_proj\": true,\n","                \"k_proj\": false,\n","                \"v_proj\": true,\n","                \"o_proj\": false,\n","                \"w1_proj\": false,\n","                \"w2_proj\": false,\n","                \"w3_proj\": false\n","            },\n","            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n","        },\n","        {\n","            \"name\": \"lora_1\",\n","            \"output\": \"lora_1\",\n","            \"optim\": \"adamw\",\n","            \"lr\": 3e-4,\n","            \"batch_size\": 16,\n","            \"micro_batch_size\": 4,\n","            \"test_batch_size\": 64,\n","            \"num_epochs\": 3,\n","            \"r\": 8,\n","            \"alpha\": 16,\n","            \"dropout\": 0.05,\n","            \"target_modules\": {\n","                \"q_proj\": true,\n","                \"k_proj\": false,\n","                \"v_proj\": true,\n","                \"o_proj\": false,\n","                \"w1_proj\": false,\n","                \"w2_proj\": false,\n","                \"w3_proj\": false\n","            },\n","            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n","        }\n","    ]\n","}\n","\"\"\"\n","\n","with open(\"./config.json\", \"w\") as f:\n","    f.write(config_string)"]},{"cell_type":"markdown","id":"4da5c1ec","metadata":{"papermill":{"duration":0.074055,"end_time":"2023-10-24T23:17:47.312386","exception":false,"start_time":"2023-10-24T23:17:47.238331","status":"completed"},"tags":[]},"source":[" ## 4. Add the path of the base model and config file path to start finetune"]},{"cell_type":"markdown","id":"11002062","metadata":{"papermill":{"duration":0.07186,"end_time":"2023-10-24T23:17:47.456571","exception":false,"start_time":"2023-10-24T23:17:47.384711","status":"completed"},"tags":[]},"source":["remember to check whether the section is on GPU. "]},{"cell_type":"code","execution_count":7,"id":"27086a9f","metadata":{"execution":{"iopub.execute_input":"2023-10-24T23:17:47.60135Z","iopub.status.busy":"2023-10-24T23:17:47.600508Z","iopub.status.idle":"2023-10-24T23:19:54.377046Z","shell.execute_reply":"2023-10-24T23:19:54.37581Z"},"papermill":{"duration":126.851231,"end_time":"2023-10-24T23:19:54.37972","exception":false,"start_time":"2023-10-24T23:17:47.528489","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\r\n","===================================BUG REPORT===================================\r\n","Welcome to bitsandbytes. For bug reports, please run\r\n","\r\n","python -m bitsandbytes\r\n","\r\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\n","================================================================================\r\n","bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\r\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\r\n","  warn(msg)\r\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\r\n","CUDA SETUP: Highest compute capability among GPUs detected: 7.5\r\n","CUDA SETUP: Detected CUDA version 118\r\n","CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\r\n","<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\r\n","[2023-10-24 23:18:03] ASPEN: NVIDIA CUDA initialized successfully.\r\n","[2023-10-24 23:18:03] ASPEN: Total 2 GPU(s) detected.\r\n","[2023-10-24 23:18:03] ASPEN: Loading model with quantization, bits = 8\r\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [01:41<00:00, 50.78s/it]\r\n","encode text data lora_0: 0/2\r\n","encode text data lora_0: 0/2\r\n","encode text data lora_1: 0/2\r\n","encode text data lora_1: 0/2\r\n","lora_1 train data:\r\n","    epoch: 1/3             step in epoch: 0/2\r\n","lora_0 train data:\r\n","    epoch: 1/3             step in epoch: 0/2\r\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\r\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n","    adapter: lora_1 loss: 2.2614712715148926\r\n","    adapter: lora_0 loss: 2.2614715099334717\r\n","lora_0 train data:\r\n","    epoch: 2/3             step in epoch: 0/2\r\n","lora_1 train data:\r\n","    epoch: 2/3             step in epoch: 0/2\r\n","    adapter: lora_0 loss: 2.2613089084625244\r\n","    adapter: lora_1 loss: 2.2613091468811035\r\n","lora_0 train data:\r\n","    epoch: 3/3             step in epoch: 0/2\r\n","lora_1 train data:\r\n","    epoch: 3/3             step in epoch: 0/2\r\n","    adapter: lora_0 loss: 2.2613089084625244\r\n","    adapter: lora_1 loss: 2.2613091468811035\r\n"]}],"source":["!python /kaggle/working/multi-lora-fine-tune/mlora.py \\\n","  --base_model /kaggle/input/llama-2/pytorch/7b-hf/1 \\\n","  --config /kaggle/working/config.json \\\n","  --load_8bit"]},{"cell_type":"markdown","id":"534e72de","metadata":{"papermill":{"duration":0.074489,"end_time":"2023-10-24T23:19:54.530874","exception":false,"start_time":"2023-10-24T23:19:54.456385","status":"completed"},"tags":[]},"source":["## 5. Then two files(lora_0, lora_1) appear in the current directory, that's the finetuned model adapter. We can download them."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":324.963262,"end_time":"2023-10-24T23:19:54.922937","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-24T23:14:29.959675","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}