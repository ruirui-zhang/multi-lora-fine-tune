{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rraydata/multi-lora-example?scriptVersionId=147764913\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"03d04647","metadata":{"papermill":{"duration":0.006926,"end_time":"2023-10-24T01:33:05.580192","exception":false,"start_time":"2023-10-24T01:33:05.573266","status":"completed"},"tags":[]},"source":["# Fine-Tuning with Llama 2, ASPEN\n","\n","Today we'll explore fine-tuning the Llama 2 model available on Kaggle Models using Multi-lora.\n","- Multi-lora: [ASPEN: Efficient LLM Model Fine-tune and Inference via Multi-Lora Optimization](https://github.com/TUDB-Labs/multi-lora-fine-tune#experiment-results) - ASPEN is an open-source framework for fine-tuning Large Language Models (LLMs) using the efficient multiple LoRA/QLoRA methods. Key features of ASPEN include: 1. Efficient LoRA/QLoRA: ASPEN optimizes the fine-tuning process, significantly reducing GPU memory usage by leveraging a shared frozen-based model.2. Multiple LoRA Adapters: Support for concurrent fine-tuning of multiple LoRA/qLoRA adapters.\n","- QloRA: [Quantized Low Rank Adapters](https://arxiv.org/abs/2305.14314): This is a method for adjusting the LLMs that uses a small number of quantified and updatable parameters to limit the complexity of the training. This technique also allows these small sets of parameters to be efficiently added to the model itself, meaning that you can make fine-tuning adjustments on many datasets, potentially, and swap these \"adapters\" in your model when necessary."]},{"cell_type":"markdown","id":"2df65498","metadata":{"papermill":{"duration":0.005628,"end_time":"2023-10-24T01:33:05.594967","exception":false,"start_time":"2023-10-24T01:33:05.589339","status":"completed"},"tags":[]},"source":["## 1. Clone multi-lora repository"]},{"cell_type":"code","execution_count":1,"id":"3913ed22","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:33:05.609077Z","iopub.status.busy":"2023-10-24T01:33:05.607653Z","iopub.status.idle":"2023-10-24T01:33:05.62075Z","shell.execute_reply":"2023-10-24T01:33:05.619236Z"},"papermill":{"duration":0.023463,"end_time":"2023-10-24T01:33:05.623953","exception":false,"start_time":"2023-10-24T01:33:05.60049","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","os.chdir('/kaggle/working/')"]},{"cell_type":"code","execution_count":2,"id":"c5cbcc37","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:33:05.638171Z","iopub.status.busy":"2023-10-24T01:33:05.636821Z","iopub.status.idle":"2023-10-24T01:33:07.658786Z","shell.execute_reply":"2023-10-24T01:33:07.657416Z"},"papermill":{"duration":2.032314,"end_time":"2023-10-24T01:33:07.662051","exception":false,"start_time":"2023-10-24T01:33:05.629737","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'multi-lora-fine-tune'...\r\n","remote: Enumerating objects: 615, done.\u001b[K\r\n","remote: Counting objects: 100% (302/302), done.\u001b[K\r\n","remote: Compressing objects: 100% (113/113), done.\u001b[K\r\n","remote: Total 615 (delta 197), reused 250 (delta 186), pack-reused 313\u001b[K\r\n","Receiving objects: 100% (615/615), 2.21 MiB | 27.98 MiB/s, done.\r\n","Resolving deltas: 100% (327/327), done.\r\n"]}],"source":["!git clone https://github.com/TUDB-Labs/multi-lora-fine-tune.git"]},{"cell_type":"markdown","id":"23c0eee9","metadata":{"papermill":{"duration":0.005889,"end_time":"2023-10-24T01:33:07.674387","exception":false,"start_time":"2023-10-24T01:33:07.668498","status":"completed"},"tags":[]},"source":["## 2. Install dependencies"]},{"cell_type":"code","execution_count":3,"id":"81e11a63","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:33:07.690758Z","iopub.status.busy":"2023-10-24T01:33:07.69029Z","iopub.status.idle":"2023-10-24T01:37:14.555698Z","shell.execute_reply":"2023-10-24T01:37:14.553207Z"},"papermill":{"duration":246.879259,"end_time":"2023-10-24T01:37:14.56047","exception":false,"start_time":"2023-10-24T01:33:07.681211","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch==2.0.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting einops==0.6.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 2))\r\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting accelerate==0.21.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3))\r\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting transformers==4.31.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4))\r\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting bitsandbytes==0.40.0 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 5))\r\n","  Downloading bitsandbytes-0.40.0-py3-none-any.whl (91.9 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 6)) (0.1.99)\r\n","Collecting scipy==1.10.1 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 7))\r\n","  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting xformers==0.0.20 (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\r\n","  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (3.2.4)\r\n","Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 10)) (0.42.1)\r\n","Collecting rouge (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 11))\r\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\r\n","Collecting rouge_chinese (from -r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 12))\r\n","  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.12.2)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (4.6.3)\r\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.12)\r\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1)\r\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (3.1.2)\r\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (1.23.5)\r\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (21.3)\r\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (5.9.3)\r\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (6.0)\r\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.16.4)\r\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.6.3)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2.31.0)\r\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.13.3)\r\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (0.3.3)\r\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (4.66.1)\r\n","Collecting pyre-extensions==0.0.29 (from xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8))\r\n","  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\r\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (68.0.0)\r\n","Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (0.40.0)\r\n","Requirement already satisfied: typing-inspect in /opt/conda/lib/python3.10/site-packages (from pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (0.9.0)\r\n","Collecting cmake (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1))\r\n","  Downloading lit-17.0.3.tar.gz (154 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 9)) (1.16.0)\r\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.9.0)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 3)) (3.0.9)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (2.1.3)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.1.0)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (3.4)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (1.26.15)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 4)) (2023.7.22)\r\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 1)) (1.3.0)\r\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20->-r /kaggle/working/multi-lora-fine-tune/requirements.txt (line 8)) (1.0.0)\r\n","Building wheels for collected packages: lit\r\n","  Building wheel for lit (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93257 sha256=7a53e5d45d12c8498154a7321f8aa4aa838f05bf89adcf6342a0cd81f4afa6eb\r\n","  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\r\n","Successfully built lit\r\n","Installing collected packages: lit, cmake, bitsandbytes, scipy, rouge_chinese, rouge, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, einops, pyre-extensions, nvidia-cusolver-cu11, nvidia-cudnn-cu11, transformers, triton, torch, xformers, accelerate\r\n","  Attempting uninstall: scipy\r\n","    Found existing installation: scipy 1.11.2\r\n","    Uninstalling scipy-1.11.2:\r\n","      Successfully uninstalled scipy-1.11.2\r\n","  Attempting uninstall: transformers\r\n","    Found existing installation: transformers 4.33.0\r\n","    Uninstalling transformers-4.33.0:\r\n","      Successfully uninstalled transformers-4.33.0\r\n","  Attempting uninstall: torch\r\n","    Found existing installation: torch 2.0.0+cpu\r\n","    Uninstalling torch-2.0.0+cpu:\r\n","      Successfully uninstalled torch-2.0.0+cpu\r\n","  Attempting uninstall: accelerate\r\n","    Found existing installation: accelerate 0.22.0\r\n","    Uninstalling accelerate-0.22.0:\r\n","      Successfully uninstalled accelerate-0.22.0\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\r\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\r\n","torchaudio 2.0.1+cpu requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\r\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\r\n","torchtext 0.15.1+cpu requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\r\n","torchvision 0.15.1+cpu requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.0 cmake-3.27.7 einops-0.6.1 lit-17.0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 rouge-1.0.1 rouge_chinese-1.0.3 scipy-1.10.1 torch-2.0.1 transformers-4.31.0 triton-2.0.0 xformers-0.0.20\r\n"]}],"source":["!pip install -r /kaggle/working/multi-lora-fine-tune/requirements.txt"]},{"cell_type":"markdown","id":"0742d399","metadata":{"papermill":{"duration":0.131659,"end_time":"2023-10-24T01:37:14.824103","exception":false,"start_time":"2023-10-24T01:37:14.692444","status":"completed"},"tags":[]},"source":["## 3. Config finetune datasets and parameters. You can add multiple lora parameters and datasets."]},{"cell_type":"markdown","id":"a4e419d6","metadata":{"papermill":{"duration":0.13046,"end_time":"2023-10-24T01:37:15.085298","exception":false,"start_time":"2023-10-24T01:37:14.954838","status":"completed"},"tags":[]},"source":["ASPEN can be used on:\n","\n","1) Domain-Specific Fine-Tuning:  This involves adapting a single model with various parameters particularly for one domain.\n","\n","2) Cross-Domain Fine-Tuning: This approach utilizes the foundational model to optimize multiple models, each designed for diverse domains, by incorporating datasets from various or identical domains.\n","\n","The demo data and prompt are for demonstration purpose."]},{"cell_type":"code","execution_count":4,"id":"ef5702f1","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:37:15.362712Z","iopub.status.busy":"2023-10-24T01:37:15.362173Z","iopub.status.idle":"2023-10-24T01:37:16.472169Z","shell.execute_reply":"2023-10-24T01:37:16.470709Z"},"papermill":{"duration":1.260536,"end_time":"2023-10-24T01:37:16.475594","exception":false,"start_time":"2023-10-24T01:37:15.215058","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[\r\n","    {\r\n","        \"instruction\": \"Instruction demo.\",\r\n","        \"input\": \"Input demo.\",\r\n","        \"output\": \"Output demo.\"\r\n","    },\r\n","    {\r\n","        \"instruction\": \"Instruction demo.\",\r\n","        \"output\": \"Output demo.\"\r\n","    }\r\n","]"]}],"source":["!cat /kaggle/working/multi-lora-fine-tune/data/data_demo.json"]},{"cell_type":"code","execution_count":5,"id":"5a328c77","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:37:16.745611Z","iopub.status.busy":"2023-10-24T01:37:16.745129Z","iopub.status.idle":"2023-10-24T01:37:17.861317Z","shell.execute_reply":"2023-10-24T01:37:17.860152Z"},"papermill":{"duration":1.253081,"end_time":"2023-10-24T01:37:17.864356","exception":false,"start_time":"2023-10-24T01:37:16.611275","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{\r\n","    \"description\": \"\",\r\n","    \"parameter\": [\r\n","        \"input\",\r\n","        \"output\",\r\n","        \"instruction\"\r\n","    ],\r\n","    \"prompt\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n{output}\\n\",\r\n","    \"prompt_no_input\": \"### Instruction:\\n{instruction}\\n\\n### Output:\\n{output}\\n\"\r\n","}"]}],"source":["!cat /kaggle/working/multi-lora-fine-tune/template/template_demo.json"]},{"cell_type":"code","execution_count":6,"id":"0f10d12b","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:37:18.130625Z","iopub.status.busy":"2023-10-24T01:37:18.130163Z","iopub.status.idle":"2023-10-24T01:37:18.140408Z","shell.execute_reply":"2023-10-24T01:37:18.139376Z"},"papermill":{"duration":0.145569,"end_time":"2023-10-24T01:37:18.143057","exception":false,"start_time":"2023-10-24T01:37:17.997488","status":"completed"},"tags":[]},"outputs":[],"source":["config_string = \"\"\"\n","{\n","    \"cutoff_len\": 256,\n","    \"group_by_length\": false,\n","    \"expand_right\": true,\n","    \"pad_token_id\": -1,\n","    \"save_step\": 2000,\n","    \"early_stop_test_step\": 2000,\n","    \"train_lora_candidate_num\": 4,\n","    \"train_lora_simultaneously_num\": 2,\n","    \"train_strategy\": \"optim\",\n","    \"lora\": [\n","        {\n","            \"name\": \"lora_0\",\n","            \"output\": \"lora_0\",\n","            \"optim\": \"adamw\",\n","            \"lr\": 3e-4,\n","            \"batch_size\": 16,\n","            \"micro_batch_size\": 4,\n","            \"test_batch_size\": 64,\n","            \"num_epochs\": 3,\n","            \"r\": 8,\n","            \"alpha\": 16,\n","            \"dropout\": 0.05,\n","            \"target_modules\": {\n","                \"q_proj\": true,\n","                \"k_proj\": false,\n","                \"v_proj\": true,\n","                \"o_proj\": false,\n","                \"w1_proj\": false,\n","                \"w2_proj\": false,\n","                \"w3_proj\": false\n","            },\n","            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n","        },\n","        {\n","            \"name\": \"lora_1\",\n","            \"output\": \"lora_1\",\n","            \"optim\": \"adamw\",\n","            \"lr\": 3e-4,\n","            \"batch_size\": 16,\n","            \"micro_batch_size\": 4,\n","            \"test_batch_size\": 64,\n","            \"num_epochs\": 3,\n","            \"r\": 8,\n","            \"alpha\": 16,\n","            \"dropout\": 0.05,\n","            \"target_modules\": {\n","                \"q_proj\": true,\n","                \"k_proj\": false,\n","                \"v_proj\": true,\n","                \"o_proj\": false,\n","                \"w1_proj\": false,\n","                \"w2_proj\": false,\n","                \"w3_proj\": false\n","            },\n","            \"data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"test_data\": \"/kaggle/working/multi-lora-fine-tune/data/data_demo.json\",\n","            \"prompt\": \"/kaggle/working/multi-lora-fine-tune/template/template_demo.json\"\n","        }\n","    ]\n","}\n","\"\"\"\n","\n","with open(\"./config.json\", \"w\") as f:\n","    f.write(config_string)"]},{"cell_type":"markdown","id":"12d9bfe2","metadata":{"papermill":{"duration":0.133088,"end_time":"2023-10-24T01:37:18.411144","exception":false,"start_time":"2023-10-24T01:37:18.278056","status":"completed"},"tags":[]},"source":[" ## 4. Add the path of the base model and config file path to start finetune"]},{"cell_type":"markdown","id":"2a8e8456","metadata":{"papermill":{"duration":0.132953,"end_time":"2023-10-24T01:37:18.678477","exception":false,"start_time":"2023-10-24T01:37:18.545524","status":"completed"},"tags":[]},"source":["remember to check whether the section is on GPU. "]},{"cell_type":"code","execution_count":7,"id":"c5f894b1","metadata":{"execution":{"iopub.execute_input":"2023-10-24T01:37:18.9458Z","iopub.status.busy":"2023-10-24T01:37:18.944694Z","iopub.status.idle":"2023-10-24T01:37:39.905874Z","shell.execute_reply":"2023-10-24T01:37:39.904345Z"},"papermill":{"duration":21.099782,"end_time":"2023-10-24T01:37:39.909186","exception":false,"start_time":"2023-10-24T01:37:18.809404","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\r\n","===================================BUG REPORT===================================\r\n","Welcome to bitsandbytes. For bug reports, please run\r\n","\r\n","python -m bitsandbytes\r\n","\r\n"," and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\n","================================================================================\r\n","bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\r\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n","  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\r\n","CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\r\n","ASPEN requires NVIDIA CUDA computing capacity. Please check your PyTorch installation.\r\n"]}],"source":["!python /kaggle/working/multi-lora-fine-tune/mlora.py \\\n","  --base_model /kaggle/input/llama-2/pytorch/7b-hf/1 \\\n","  --config /kaggle/working/config.json \\\n","  --load_8bit"]},{"cell_type":"markdown","id":"5d5ad080","metadata":{"papermill":{"duration":0.13288,"end_time":"2023-10-24T01:37:40.182472","exception":false,"start_time":"2023-10-24T01:37:40.049592","status":"completed"},"tags":[]},"source":["## 5. Then two files(lora_0, lora_1) appear in the current directory, that's the finetuned model adapter. We can download them."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":279.557184,"end_time":"2023-10-24T01:37:40.966483","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-24T01:33:01.409299","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}